{"cells":[{"cell_type":"markdown","metadata":{"id":"rDAsHX24_q68"},"source":["# Loss Function, Optimization 과제\n","> 인공지능 스터디 세 번째 과제에 오신 것을 환영합니다! 강의를 들으면서 배운 다양한 지식들을 실습을 통해서 활용해 볼 시간을 가질 것입니다!"]},{"cell_type":"markdown","metadata":{"id":"5fO7KUlh_q6_"},"source":["🐙<br>\n","이번 과제는 퀴즈와 실습으로 이루어져 있어요!"]},{"cell_type":"markdown","metadata":{"id":"Ub0i6std_q6_"},"source":["#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Cross Entropy Loss (단일선택)\n","```python\n","Cross Entropy Loss 함수가 주로 사용되는 경우는?\n","\n","\n","(1) 연속적인 값을 예측하는 회귀 문제\n","(2) 이진 분류 문제\n","(3) 군집화 문제\n","(4) 시계열 예측 문제\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"t9NzARf0_q7A"},"source":["```python\n","😉\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{"id":"wKMFQ1V1_q7A"},"source":["#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Regularization (단일선택)\n","```\n","L1 Regularization(Lasso)의 특징으로 올바른 것은?\n","\n","(1) 모든 특성의 가중치를 균등하게 감소시킨다\n","(2) 특성 선택에 효과가 있다\n","(3) 항상 L2 Regularization보다 좋은 성능을 낸다\n","(4) 미분이 불가능한 지점이 없어 최적화가 쉽다\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"j-KuV37Q_q7A"},"source":["```python\n","😉\n","[2]\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class LossFunction:\n","    \"\"\"손실 함수를 계산하고 시각화하는 클래스\"\"\"\n","    \n","    def __init__(self, label: bool = True) -> None:\n","        \"\"\"\n","        LossFunction 클래스의 생성자\n","        \n","        :param label: True는 양성 클래스, False는 음성 클래스를 나타냄\n","        \"\"\"\n","        self.label = label\n","        # 0.001부터 1.0까지 1000개의 균일한 간격의 확률값 생성\n","        self.p = np.linspace(start=0.001, stop=1.0, num=1000)\n","    \n","    def get_cross_entropy(self) -> np.ndarray:\n","        \"\"\"\n","        Binary Cross Entropy Loss 계산\n","        \n","        :return: 각 확률값에 대한 Cross Entropy Loss 배열\n","        \"\"\"\n","        # label이 True일 때와 False일 때의 계산식이 다름. 강의자료 참고\n","        return -1 * np.log2(self.p) if self.label else -1 * np.log2(1 - self.p)\n","    \n","    def get_squared_error(self) -> np.ndarray:\n","        \"\"\"\n","        Squared Error Loss 계산\n","        \n","        :return: 각 확률값에 대한 Squared Error Loss 배열\n","        \"\"\"\n","        # label이 True일 때와 False일 때의 계산식이 다름\n","        return np.square(1 - self.p) if self.label else np.square(self.p)\n","    \n","    def plot_loss(self, loss_type: str) -> None:\n","        \"\"\"\n","        지정된 손실 함수의 그래프를 그림\n","        \n","        :param loss_type: 'Cross Entropy' 또는 'Squared Error'\n","        \"\"\"\n","        plt.figure(figsize=(10, 6))  # 그래프 크기 설정\n","        plt.xlabel('Probability')    # x축 레이블\n","        plt.ylabel('Loss')           # y축 레이블\n","        plt.title(f'{loss_type} Loss (Label: {self.label})')  # 그래프 제목\n","        \n","        # 손실 함수 유형에 따라 적절한 메서드 호출\n","        if loss_type == 'Cross Entropy':\n","            loss = self.get_cross_entropy()\n","        elif loss_type == 'Squared Error':\n","            loss = self.get_squared_error()\n","        else:\n","            raise ValueError(\"Invalid loss type. Use 'Cross Entropy' or 'Squared Error'.\")\n","        \n","        plt.plot(self.p, loss)  # 그래프 그리기\n","        plt.show()              # 그래프 표시\n","\n","def compare_losses(label: bool) -> None:\n","    \"\"\"\n","    Cross Entropy Loss와 Squared Error Loss를 같은 그래프에 그려 비교\n","    \n","    :param label: True는 양성 클래스, False는 음성 클래스를 나타냄\n","    \"\"\"\n","    loss_func = LossFunction(label)\n","    \n","    plt.figure(figsize=(12, 6))  # 그래프 크기 설정\n","    plt.xlabel('Probability')    # x축 레이블\n","    plt.ylabel('Loss')           # y축 레이블\n","    plt.title(f'Cross Entropy vs Squared Error Loss (Label: {label})')\n","    \n","    # 두 손실 함수의 그래프를 같은 평면에 그림\n","    plt.plot(loss_func.p, loss_func.get_cross_entropy(), label='Cross Entropy')\n","    plt.plot(loss_func.p, loss_func.get_squared_error(), label='Squared Error')\n","    \n","    plt.legend()  # 범례 표시\n","    plt.show()    # 그래프 표시\n","\n","# 실습 1: Cross Entropy Loss 시각화\n","loss_func_true = LossFunction(True)   # 양성 클래스(label=True)에 대한 LossFunction 객체 생성\n","loss_func_true.plot_loss('Cross Entropy')  # 양성 클래스의 Cross Entropy Loss 그래프 그리기\n","\n","loss_func_false = LossFunction(False)  # 음성 클래스(label=False)에 대한 LossFunction 객체 생성\n","loss_func_false.plot_loss('Cross Entropy')  # 음성 클래스의 Cross Entropy Loss 그래프 그리기"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["compare_losses(True)\n","compare_losses(False)"]},{"cell_type":"markdown","metadata":{},"source":["#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Squared Error Loss와 Cross Entropy Loss (단일선택)\n","```\n","Label이 True일 때, Squared Error Loss와 Cross Entropy Loss를 비교했을 때 어느 쪽이 극단적인 오류(예측이 실제와 매우 다른 경우)에 대해 더 큰 페널티를 부여하나요?\n","\n","(1) Squared Error Loss\n","(2) Cross Entropy Loss\n","(3) 둘 다 동일한 페널티를 부여한다\n","(4) 예측 확률에 따라 다르다\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","😉\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Squared Error Loss와 Cross Entropy Loss (주관식)\n","```\n","Cross Entropy Loss와 Squared Error Loss 중 어느 것이 이진 분류 문제에서 더 자주 사용되며, 그 이유는 무엇인가요?\n","\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","😉\n","Cross Entropy Loss, 이진 분류에서 우리의 목표는 주어진 입력에 대해 각 클래스에 속할 확률을 예측하는 것인데 Cross Entropy는 예측된 확률 분포와 실제 확률 분포 간의 차이를 측정하는 데 최적화되어있게때문에 더 적합하다.\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> 정규화(Regularization)의 주요 목적은 무엇인가요? (단일선택)\n","```\n","(1) 모델의 학습 속도를 높이는 것\n","(2) 모델의 복잡성을 줄이고 과적합을 방지하는 것\n","(3) 모델의 파라미터 수를 증가시키는 것\n","(4) 데이터셋의 크기를 확장하는 것\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","😉\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### ❓ <font color='red'><b>[ 참고자료 ]</b></font> 참고자료\n","\n","추가적으로 optimization 기법 중 AdamW 기법에 대한 잘 설명해놓은 글이 있어 첨부해놓았습니다. 논문 리뷰라서 어려울 수도 있지만, 궁금하시면 한번씩 읽어보세요!\n","\n","https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html"]},{"cell_type":"markdown","metadata":{"id":"ZjErdk6W_q7C"},"source":["### 🎉🎉🎉 4주차 과제 완료! 🎉🎉🎉\n","```python\n","🐙\n","여러분 모두 수고 했어요!!\n","```"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}

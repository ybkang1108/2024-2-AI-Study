{"cells":[{"cell_type":"markdown","metadata":{"id":"rDAsHX24_q68"},"source":["# Loss Function, Optimization ê³¼ì œ\n","> ì¸ê³µì§€ëŠ¥ ìŠ¤í„°ë”” ì„¸ ë²ˆì§¸ ê³¼ì œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ë°°ìš´ ë‹¤ì–‘í•œ ì§€ì‹ë“¤ì„ ì‹¤ìŠµì„ í†µí•´ì„œ í™œìš©í•´ ë³¼ ì‹œê°„ì„ ê°€ì§ˆ ê²ƒì…ë‹ˆë‹¤!"]},{"cell_type":"markdown","metadata":{"id":"5fO7KUlh_q6_"},"source":["ğŸ™<br>\n","ì´ë²ˆ ê³¼ì œëŠ” í€´ì¦ˆì™€ ì‹¤ìŠµìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ìš”!"]},{"cell_type":"markdown","metadata":{"id":"Ub0i6std_q6_"},"source":["#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Cross Entropy Loss (ë‹¨ì¼ì„ íƒ)\n","```python\n","Cross Entropy Loss í•¨ìˆ˜ê°€ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ëŠ”?\n","\n","\n","(1) ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ë¬¸ì œ\n","(2) ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ\n","(3) êµ°ì§‘í™” ë¬¸ì œ\n","(4) ì‹œê³„ì—´ ì˜ˆì¸¡ ë¬¸ì œ\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"t9NzARf0_q7A"},"source":["```python\n","ğŸ˜‰\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{"id":"wKMFQ1V1_q7A"},"source":["#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Regularization (ë‹¨ì¼ì„ íƒ)\n","```\n","L1 Regularization(Lasso)ì˜ íŠ¹ì§•ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?\n","\n","(1) ëª¨ë“  íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê· ë“±í•˜ê²Œ ê°ì†Œì‹œí‚¨ë‹¤\n","(2) íŠ¹ì„± ì„ íƒì— íš¨ê³¼ê°€ ìˆë‹¤\n","(3) í•­ìƒ L2 Regularizationë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤\n","(4) ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•œ ì§€ì ì´ ì—†ì–´ ìµœì í™”ê°€ ì‰½ë‹¤\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"j-KuV37Q_q7A"},"source":["```python\n","ğŸ˜‰\n","[2]\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class LossFunction:\n","    \"\"\"ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n","    \n","    def __init__(self, label: bool = True) -> None:\n","        \"\"\"\n","        LossFunction í´ë˜ìŠ¤ì˜ ìƒì„±ì\n","        \n","        :param label: TrueëŠ” ì–‘ì„± í´ë˜ìŠ¤, FalseëŠ” ìŒì„± í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ„\n","        \"\"\"\n","        self.label = label\n","        # 0.001ë¶€í„° 1.0ê¹Œì§€ 1000ê°œì˜ ê· ì¼í•œ ê°„ê²©ì˜ í™•ë¥ ê°’ ìƒì„±\n","        self.p = np.linspace(start=0.001, stop=1.0, num=1000)\n","    \n","    def get_cross_entropy(self) -> np.ndarray:\n","        \"\"\"\n","        Binary Cross Entropy Loss ê³„ì‚°\n","        \n","        :return: ê° í™•ë¥ ê°’ì— ëŒ€í•œ Cross Entropy Loss ë°°ì—´\n","        \"\"\"\n","        # labelì´ Trueì¼ ë•Œì™€ Falseì¼ ë•Œì˜ ê³„ì‚°ì‹ì´ ë‹¤ë¦„. ê°•ì˜ìë£Œ ì°¸ê³ \n","        return -1 * np.log2(self.p) if self.label else -1 * np.log2(1 - self.p)\n","    \n","    def get_squared_error(self) -> np.ndarray:\n","        \"\"\"\n","        Squared Error Loss ê³„ì‚°\n","        \n","        :return: ê° í™•ë¥ ê°’ì— ëŒ€í•œ Squared Error Loss ë°°ì—´\n","        \"\"\"\n","        # labelì´ Trueì¼ ë•Œì™€ Falseì¼ ë•Œì˜ ê³„ì‚°ì‹ì´ ë‹¤ë¦„\n","        return np.square(1 - self.p) if self.label else np.square(self.p)\n","    \n","    def plot_loss(self, loss_type: str) -> None:\n","        \"\"\"\n","        ì§€ì •ëœ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼\n","        \n","        :param loss_type: 'Cross Entropy' ë˜ëŠ” 'Squared Error'\n","        \"\"\"\n","        plt.figure(figsize=(10, 6))  # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •\n","        plt.xlabel('Probability')    # xì¶• ë ˆì´ë¸”\n","        plt.ylabel('Loss')           # yì¶• ë ˆì´ë¸”\n","        plt.title(f'{loss_type} Loss (Label: {self.label})')  # ê·¸ë˜í”„ ì œëª©\n","        \n","        # ì†ì‹¤ í•¨ìˆ˜ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë©”ì„œë“œ í˜¸ì¶œ\n","        if loss_type == 'Cross Entropy':\n","            loss = self.get_cross_entropy()\n","        elif loss_type == 'Squared Error':\n","            loss = self.get_squared_error()\n","        else:\n","            raise ValueError(\"Invalid loss type. Use 'Cross Entropy' or 'Squared Error'.\")\n","        \n","        plt.plot(self.p, loss)  # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n","        plt.show()              # ê·¸ë˜í”„ í‘œì‹œ\n","\n","def compare_losses(label: bool) -> None:\n","    \"\"\"\n","    Cross Entropy Lossì™€ Squared Error Lossë¥¼ ê°™ì€ ê·¸ë˜í”„ì— ê·¸ë ¤ ë¹„êµ\n","    \n","    :param label: TrueëŠ” ì–‘ì„± í´ë˜ìŠ¤, FalseëŠ” ìŒì„± í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ëƒ„\n","    \"\"\"\n","    loss_func = LossFunction(label)\n","    \n","    plt.figure(figsize=(12, 6))  # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •\n","    plt.xlabel('Probability')    # xì¶• ë ˆì´ë¸”\n","    plt.ylabel('Loss')           # yì¶• ë ˆì´ë¸”\n","    plt.title(f'Cross Entropy vs Squared Error Loss (Label: {label})')\n","    \n","    # ë‘ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ë¥¼ ê°™ì€ í‰ë©´ì— ê·¸ë¦¼\n","    plt.plot(loss_func.p, loss_func.get_cross_entropy(), label='Cross Entropy')\n","    plt.plot(loss_func.p, loss_func.get_squared_error(), label='Squared Error')\n","    \n","    plt.legend()  # ë²”ë¡€ í‘œì‹œ\n","    plt.show()    # ê·¸ë˜í”„ í‘œì‹œ\n","\n","# ì‹¤ìŠµ 1: Cross Entropy Loss ì‹œê°í™”\n","loss_func_true = LossFunction(True)   # ì–‘ì„± í´ë˜ìŠ¤(label=True)ì— ëŒ€í•œ LossFunction ê°ì²´ ìƒì„±\n","loss_func_true.plot_loss('Cross Entropy')  # ì–‘ì„± í´ë˜ìŠ¤ì˜ Cross Entropy Loss ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n","\n","loss_func_false = LossFunction(False)  # ìŒì„± í´ë˜ìŠ¤(label=False)ì— ëŒ€í•œ LossFunction ê°ì²´ ìƒì„±\n","loss_func_false.plot_loss('Cross Entropy')  # ìŒì„± í´ë˜ìŠ¤ì˜ Cross Entropy Loss ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["compare_losses(True)\n","compare_losses(False)"]},{"cell_type":"markdown","metadata":{},"source":["#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Squared Error Lossì™€ Cross Entropy Loss (ë‹¨ì¼ì„ íƒ)\n","```\n","Labelì´ Trueì¼ ë•Œ, Squared Error Lossì™€ Cross Entropy Lossë¥¼ ë¹„êµí–ˆì„ ë•Œ ì–´ëŠ ìª½ì´ ê·¹ë‹¨ì ì¸ ì˜¤ë¥˜(ì˜ˆì¸¡ì´ ì‹¤ì œì™€ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš°)ì— ëŒ€í•´ ë” í° í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ë‚˜ìš”?\n","\n","(1) Squared Error Loss\n","(2) Cross Entropy Loss\n","(3) ë‘˜ ë‹¤ ë™ì¼í•œ í˜ë„í‹°ë¥¼ ë¶€ì—¬í•œë‹¤\n","(4) ì˜ˆì¸¡ í™•ë¥ ì— ë”°ë¼ ë‹¤ë¥´ë‹¤\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","ğŸ˜‰\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Squared Error Lossì™€ Cross Entropy Loss (ì£¼ê´€ì‹)\n","```\n","Cross Entropy Lossì™€ Squared Error Loss ì¤‘ ì–´ëŠ ê²ƒì´ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë” ìì£¼ ì‚¬ìš©ë˜ë©°, ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n","\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","ğŸ˜‰\n","Cross Entropy Loss, ì´ì§„ ë¶„ë¥˜ì—ì„œ ìš°ë¦¬ì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì¸ë° Cross EntropyëŠ” ì˜ˆì¸¡ëœ í™•ë¥  ë¶„í¬ì™€ ì‹¤ì œ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ìˆê²Œë•Œë¬¸ì— ë” ì í•©í•˜ë‹¤.\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> ì •ê·œí™”(Regularization)ì˜ ì£¼ìš” ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”? (ë‹¨ì¼ì„ íƒ)\n","```\n","(1) ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ê²ƒ\n","(2) ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ê²ƒ\n","(3) ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒ\n","(4) ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ í™•ì¥í•˜ëŠ” ê²ƒ\n","\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","ğŸ˜‰\n","[2]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["#### â“ <font color='red'><b>[ ì°¸ê³ ìë£Œ ]</b></font> ì°¸ê³ ìë£Œ\n","\n","ì¶”ê°€ì ìœ¼ë¡œ optimization ê¸°ë²• ì¤‘ AdamW ê¸°ë²•ì— ëŒ€í•œ ì˜ ì„¤ëª…í•´ë†“ì€ ê¸€ì´ ìˆì–´ ì²¨ë¶€í•´ë†“ì•˜ìŠµë‹ˆë‹¤. ë…¼ë¬¸ ë¦¬ë·°ë¼ì„œ ì–´ë ¤ìš¸ ìˆ˜ë„ ìˆì§€ë§Œ, ê¶ê¸ˆí•˜ì‹œë©´ í•œë²ˆì”© ì½ì–´ë³´ì„¸ìš”!\n","\n","https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html"]},{"cell_type":"markdown","metadata":{"id":"ZjErdk6W_q7C"},"source":["### ğŸ‰ğŸ‰ğŸ‰ 4ì£¼ì°¨ ê³¼ì œ ì™„ë£Œ! ğŸ‰ğŸ‰ğŸ‰\n","```python\n","ğŸ™\n","ì—¬ëŸ¬ë¶„ ëª¨ë‘ ìˆ˜ê³  í–ˆì–´ìš”!!\n","```"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
